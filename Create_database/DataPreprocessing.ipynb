{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI6Sslb3DiKw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_imdb_data(input_file_path, output_file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file_path, delimiter='\\t', low_memory=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read the TSV file: {e}\")\n",
        "        return\n",
        "    if df.empty:\n",
        "        print(\"The DataFrame is empty.\")\n",
        "        return\n",
        "    if 'directors' not in df.columns:\n",
        "        print(\"directors column is missing in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    df = df[df['directors'] != '\\\\N']\n",
        "\n",
        "    df_genres = df['directors'].str.split(',', expand=True).stack().reset_index(level=1, drop=True)\n",
        "    df_genres.name = 'director'\n",
        "\n",
        "    df_normalized = df.drop('directors', axis=1).join(df_genres)[['tconst', 'director']]\n",
        "\n",
        "    try:\n",
        "        df_normalized.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write the compressed TSV file: {e}\")\n",
        "\n",
        "path_to_input_file = '/content/title.crew.tsv.gz'\n",
        "path_to_output_file = 'directors.tsv.gz'\n",
        "\n",
        "process_imdb_data(path_to_input_file, path_to_output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKG46tqqbbOO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO7n3BPwbbhT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_imdb_data(input_file_path, output_file_path):\n",
        "    chunk_size = 50000\n",
        "    try:\n",
        "        with open(output_file_path, 'w', newline='') as file_out:\n",
        "            header = True\n",
        "\n",
        "            for i, df in enumerate(pd.read_csv(input_file_path, delimiter='\\t', low_memory=False, chunksize=chunk_size)):\n",
        "                print(f\"Processing chunk {i+1}\")\n",
        "\n",
        "                df = df[df['writers'] != '\\\\N']\n",
        "\n",
        "                df_expanded = df['writers'].str.split(',', expand=True).stack().reset_index(level=1, drop=True)\n",
        "                df_expanded.name = 'writer'\n",
        "\n",
        "                df_normalized = df.drop('writers', axis=1).join(df_expanded)[['tconst', 'writer']]\n",
        "\n",
        "\n",
        "                df_normalized.to_csv(file_out, sep='\\t', index=False, header=header, mode='a')\n",
        "                header = False\n",
        "\n",
        "\n",
        "                print(f\"Chunk {i+1} processed: {df_normalized.shape[0]} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during processing: {e}\")\n",
        "\n",
        "\n",
        "path_to_input_file = '/content/title.crew.tsv.gz'\n",
        "path_to_output_file = 'writers.tsv.gz'\n",
        "\n",
        "process_imdb_data(path_to_input_file, path_to_output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qIDxwGvbbrG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc2QT4-gbbvO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_imdb_data(input_file_path, output_file_path):\n",
        "    try:\n",
        "\n",
        "        df = pd.read_csv(input_file_path, delimiter='\\t', low_memory=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read the TSV file: {e}\")\n",
        "        return\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"The DataFrame is empty.\")\n",
        "        return\n",
        "    if 'knownForTitles' not in df.columns:\n",
        "        print(\"knownForTitles column is missing in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    df = df[df['knownForTitles'] != '\\\\N']\n",
        "\n",
        "    df_genres = df['knownForTitles'].str.split(',', expand=True).stack().reset_index(level=1, drop=True)\n",
        "    df_genres.name = 'knownForTitles'\n",
        "\n",
        "    df_normalized = df.drop('knownForTitles', axis=1).join(df_genres)[['nconst', 'knownForTitle']]\n",
        "\n",
        "    try:\n",
        "        df_normalized.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write the compressed TSV file: {e}\")\n",
        "\n",
        "path_to_input_file = '/content/name.basics.tsv.gz'\n",
        "path_to_output_file = 'knownForTitles.tsv.gz'\n",
        "\n",
        "process_imdb_data(path_to_input_file, path_to_output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXK-ZhcffNpN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHc4HtAPOGQ-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_directors(input_file_path, output_file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file_path, delimiter='\\t', low_memory=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read the TSV file: {e}\")\n",
        "        return\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"The DataFrame is empty.\")\n",
        "        return\n",
        "    if 'directors' not in df.columns:\n",
        "        print(\"Directors column is missing in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    df = df[df['directors'] != '\\\\N']\n",
        "\n",
        "    df_directors = df['directors'].str.split(',', expand=True).stack().reset_index(level=1, drop=True)\n",
        "    df_directors.name = 'director'\n",
        "\n",
        "    df_normalized = df.drop('directors', axis=1).join(df_directors)[['tconst', 'director']]\n",
        "\n",
        "    try:\n",
        "        df_normalized.to_csv(output_file_path, sep='\\t', index=False, compression='gzip')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write the compressed TSV file: {e}\")\n",
        "\n",
        "path_to_input_file = '/content/title.crew.tsv'\n",
        "path_to_output_file = 'directors.tsv.gz'\n",
        "\n",
        "process_directors(path_to_input_file, path_to_output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def process_akas_data(input_file_path, output_file_path):\n",
        "    chunk_size = 50000  # Adjust the chunk size based on your RAM constraints\n",
        "    try:\n",
        "        with gzip.open(output_file_path, 'wt') as file_out:  # Use gzip for writing compressed output\n",
        "            header = True  # To write header only once\n",
        "            # Process the file in chunks\n",
        "            for i, df in enumerate(pd.read_csv(input_file_path, delimiter='\\t', low_memory=False, chunksize=chunk_size)):\n",
        "                print(f\"Processing chunk {i+1}\")\n",
        "\n",
        "                # Handle 'types' and 'attributes' columns by taking the first element of the split\n",
        "                if 'types' in df.columns:\n",
        "                    df['types'] = df['types'].apply(lambda x: x.split(',')[0] if pd.notna(x) else x)\n",
        "                if 'attributes' in df.columns:\n",
        "                    df['attributes'] = df['attributes'].apply(lambda x: x.split(',')[0] if pd.notna(x) else x)\n",
        "\n",
        "                # Write processed chunk to file with compression\n",
        "                df.to_csv(file_out, sep='\\t', index=False, header=header)\n",
        "                header = False  # Disable header after the first chunk\n",
        "\n",
        "                # Provide some output about the process\n",
        "                print(f\"Chunk {i+1} processed: {df.shape[0]} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during processing: {e}\")\n",
        "\n",
        "# Define the paths\n",
        "path_to_input_file = '/content/title.akas.tsv.gz'  # Akas file path\n",
        "path_to_output_file = 'processed_akas.tsv.gz'  # Output file path\n",
        "\n",
        "# Process the data\n",
        "process_akas_data(path_to_input_file, path_to_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPafs1IH6uZ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_K1Itv46uce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def load_valid_tconsts(basics_file_path):\n",
        "    with gzip.open(basics_file_path, 'rt') as file:\n",
        "        df_basics = pd.read_csv(file, usecols=['tconst'], delimiter='\\t', low_memory=False)\n",
        "    return pd.Index(df_basics['tconst'])\n",
        "\n",
        "def process_chunk(df_chunk, valid_tconsts, file_out_path, is_first_chunk):\n",
        "    df_chunk = df_chunk[df_chunk['tconst'].isin(valid_tconsts) & (df_chunk['writers'] != '\\\\N')]\n",
        "    df_expanded = df_chunk['writers'].str.split(',', expand=True).stack().reset_index(level=1, drop=True)\n",
        "    df_expanded.name = 'writer'\n",
        "\n",
        "    df_normalized = df_chunk[['tconst']].join(df_expanded)\n",
        "\n",
        "    with gzip.open(file_out_path, 'at' if not is_first_chunk else 'wt') as file_out:\n",
        "        df_normalized.to_csv(file_out, sep='\\t', index=False, header=is_first_chunk)\n",
        "    return df_normalized.shape[0]\n",
        "\n",
        "def process_writers(input_file_path, output_file_path, valid_tconsts):\n",
        "    chunk_size = 50000\n",
        "    with pd.read_csv(input_file_path, delimiter='\\t', low_memory=False, chunksize=chunk_size) as reader:\n",
        "        with Pool(processes=4) as pool:\n",
        "            results = [pool.apply_async(process_chunk, (df, valid_tconsts, output_file_path, i == 0))\n",
        "                       for i, df in enumerate(reader)]\n",
        "\n",
        "            for result in results:\n",
        "                rows_processed = result.get()\n",
        "                print(f\"Chunk processed: {rows_processed} rows\")\n",
        "\n",
        "path_to_basics_file = '/content/title.basics.tsv.gz'\n",
        "path_to_input_file = '/content/title.crew.tsv.gz'\n",
        "path_to_output_file = 'writers.tsv.gz'\n",
        "\n",
        "valid_tconsts = load_valid_tconsts(path_to_basics_file)\n",
        "\n",
        "process_writers(path_to_input_file, path_to_output_file, valid_tconsts)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
